{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: [WinError 10054] An existing\n",
      "[nltk_data]     connection was forcibly closed by the remote host\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\babenyshevs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/gen_councelor.txt'\n",
    "folder_path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords with frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>genetic_counseling</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>genetic_counselor</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>american_board</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american_board_genetic</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>board_genetic</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  keyword  frequency\n",
       "0      genetic_counseling         21\n",
       "1       genetic_counselor         13\n",
       "2          american_board         12\n",
       "3  american_board_genetic          7\n",
       "4           board_genetic          7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "class KeywordExtractor:\n",
    "    def __init__(self, file_path, folder_path=None):\n",
    "        self.file_path = file_path\n",
    "        if folder_path is not None:\n",
    "            self.text = self._read_folder(folder_path)\n",
    "        else:\n",
    "            self.text = self._read_file(file_path)\n",
    "        self.processed_text = self._process_text(self.text)\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _read_folder(self, folder_path):\n",
    "        _text = \"\"\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                _text += self._read_file(file_path)\n",
    "        return _text\n",
    "\n",
    "    def _process_text(self, text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.extend([\"experience\", \"product\", \"team\"])\n",
    "        filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def _extract_keywords(self, n):\n",
    "        processed_tokens = self._process_text(self.text)\n",
    "        n_grams = list(ngrams(processed_tokens, n))\n",
    "        n_gram_frequencies = Counter(n_grams)\n",
    "        _keyword_frequency = n_gram_frequencies.most_common()\n",
    "        return _keyword_frequency\n",
    "\n",
    "    def extract_keywords(self, min_n=1, max_n=5, drop_duplicate=True):\n",
    "        self.keyword_frequency = {}\n",
    "        for n in reversed(range(min_n,max_n)):\n",
    "            ngram_keywords = self._extract_keywords(n)\n",
    "            for key_freq_pair in ngram_keywords:\n",
    "                n_key = \"_\".join(key_freq_pair[0]) # normalize ngram text\n",
    "                n_freq = key_freq_pair[1]\n",
    "                keys = self.keyword_frequency.keys()\n",
    "                if drop_duplicate:\n",
    "                    if sum([n_key in key for key in keys]) == 0: # not presented even partially in other keys\n",
    "                        self.keyword_frequency[n_key] = n_freq\n",
    "                else:\n",
    "                    self.keyword_frequency[n_key] = n_freq\n",
    "\n",
    "    def display_keywords(self, top_n=10):\n",
    "        print(\"Keywords with frequencies:\")\n",
    "        df_keywords = pd.DataFrame(self.keyword_frequency.items(), \n",
    "                                   columns=[\"keyword\",\"frequency\"]).sort_values(\"frequency\", ascending=False).reset_index(drop=True)\n",
    "        display(df_keywords.head(top_n))\n",
    "\n",
    "    def draw_wordcloud(self):\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(self.keyword_frequency)\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ke = KeywordExtractor(\n",
    "                        file_path=file_path, \n",
    "                        # folder_path=folder_path\n",
    "                        )\n",
    "    ke.extract_keywords(2, 4, False)\n",
    "    ke.display_keywords(5)\n",
    "    # ke.draw_wordcloud()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
